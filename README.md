# multi-agent_llm
## A multi-agent, multimodal system that can incorporate RAG, memory and planning; and can take input from other traditional ML algorithms (e.g. CNNs).

The present repository is based on previous and ongoing work at the Okinawa Institute of Science and Technology (OIST). A multi-agent system was developed, which coordinated several LLMs, a fine-tuned Convolutional Neural Network for image detection, and a robotic arm. The system was designed to simulate a teaching environment, where an AI agent took on the role of a mother guiding a child in manipulating colored cubes. The system was capable of real-time communication in open natural language (speech), object detection, reasoning, memory, and improvisation.  

The following video comes from a longer human-robot interaction. It illustrates the open-language, reasoning capabilties of the system. Notice how the system succesfully accomplishes a task based on indirect instructions (to point with the robotic arm at a block that is the "color of the sky").

https://github.com/user-attachments/assets/aafd5a66-2225-416d-8d04-1f99cc9c2c8e

